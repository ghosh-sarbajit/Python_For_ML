{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d50436b",
   "metadata": {},
   "source": [
    "### Sri Sri Durga Saranyam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5772e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import of required modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time as t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea524b0",
   "metadata": {},
   "source": [
    "## Q1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c23f9cad",
   "metadata": {},
   "source": [
    "### Q1.a Finding min of two given function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf14a3",
   "metadata": {},
   "source": [
    "Given two functions are:\n",
    "1. $f(x) = x^2 + 3x + 4$\n",
    "2. $f(x) = x^4 - 3x^2 + 2x$\n",
    "\n",
    "First order derivative of given two functions are:\n",
    "1. $f(x) = x^2 + 3x + 4$, $ f^\\prime(x) = 2x + 3$\n",
    "2. $f(x) = x^4 - 3x^2 + 2x$, $ f^\\prime(x) = 4x^3 - 6x + 2$\n",
    "\n",
    "Note the first function is a parabola hence it is a convex function. So it does not matter from which point we will start, it will eventually converge into global minima.\n",
    "\n",
    "For the second function we will plot its graph to know nature of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06236116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value of function 1 is: 1.75 and this is attained at: -1.5\n"
     ]
    }
   ],
   "source": [
    "# Finding min for function1\n",
    "def gradient_descent(gradient, init_, learn_rate, n_iter=50, tol=1e-06):\n",
    "    x = init_\n",
    "    # print(init_)\n",
    "    for _ in range(n_iter):\n",
    "        # print(gradient)\n",
    "        delta = -learn_rate * gradient(x)\n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "        x += delta\n",
    "    return round(x*1000)/1000\n",
    "\n",
    "# function 1\n",
    "def func1(x):\n",
    "    return ((x*x) + (3*x) + 4)\n",
    "\n",
    "# gradient1 is for calculation for function 1\n",
    "def gradient1(x):\n",
    "    return (2*x + 3)\n",
    "\n",
    "min_val_at = gradient_descent(gradient1, 0, 0.5)\n",
    "\n",
    "print(\"Min value of function 1 is: \" + str(func1(min_val_at)) + \" and this is attained at: \" + str(min_val_at))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "682f2875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-15.0, 30.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x480 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# gradient function for function 2\n",
    "def gradient2(x):\n",
    "    return ((4*x*x*x) - (6*x) + 2)\n",
    "\n",
    "# function 2\n",
    "def func2(x):\n",
    "    return (x**4 - 3*x**2 + 2*x)\n",
    "\n",
    "x_list = np.linspace(-3,3, 5000)\n",
    "y_list = func2(x_list)\n",
    "# plt.figure?\n",
    "plt.figure(0, dpi = 120)\n",
    "# plt.plot?\n",
    "plt.plot(x_list, y_list)\n",
    "plt.ylim(-15,30) # setting limit of y-axis bottom = -15; top = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68a889d",
   "metadata": {},
   "source": [
    "So from the graph we can see that this function has global minimua some where between **[-2,-1]**. So we will choose **-2** as our initial choice to run the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60c4adfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min value of function 2 is: -4.848076206064 and this is attained at: -1.366\n"
     ]
    }
   ],
   "source": [
    "min_val_at = gradient_descent(gradient2, -2, 0.025)\n",
    "# print(min_val_at)\n",
    "\n",
    "# Finding minima value for function 2\n",
    "print(\"Min value of function 2 is: \" + str(func2(min_val_at)) + \" and this is attained at: \" + str(min_val_at))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb44bf45",
   "metadata": {},
   "source": [
    "### Q1.b Derivation of update rule for lin reg, while using gradient descent method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a998f921",
   "metadata": {},
   "source": [
    "Let us consider we have data, $\\{(x_1,y_1), \\dots, (x_n, y_n)\\}$.\n",
    "\n",
    "We want to fit linear regression into this data.So, we are interested in the relation $y=ax+b$.\n",
    "\n",
    "So in order to fit the above relation we have to minimize the mean square error, which is given by $$S = \\frac{1}{n} \\sum_{i=1}^n (y_i - y_{pred})^2 = \\frac{1}{n} \\sum_{i=1}^n (y_i - ax_i - b)^2$$ by finding suitable $(a, b)$ which will minimize the error.\n",
    "\n",
    "We will minimize the above objective function using gradient descent method. Let us first calculate the partial derivativs wrt $a$ and $b$ respectively.\n",
    "\n",
    "$\\begin{align*}\n",
    "    D_a & =\n",
    "    \\frac{\\partial S}{\\partial a} \\\\\n",
    "     & = \\frac{\\partial}{\\partial a} \\frac{1}{n} \\sum_{i=1}^n (y_i - ax_i - b)^2 \\\\\n",
    "     & = - \\frac{2}{n} \\sum_{i=1}^n x_i(y_i - ax_i - b) \\\\\n",
    "     & = - \\frac{2}{n} \\sum_{i=1}^n x_i(y_i - y_{pred})\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "    D_b & =\n",
    "    \\frac{\\partial S}{\\partial b} \\\\\n",
    "     & = \\frac{\\partial}{\\partial b} \\frac{1}{n} \\sum_{i=1}^n (y_i - ax_i - b)^2 \\\\\n",
    "     & = - \\frac{2}{n} \\sum_{i=1}^n (y_i - ax_i - b) \\\\\n",
    "     & = - \\frac{2}{n} \\sum_{i=1}^n (y_i - y_{pred})\n",
    "\\end{align*}$\n",
    "\n",
    "Hence the update rule will be as follows:\n",
    "$$a:=a- \\lambda D_a$$\n",
    "$$b:=b -\\lambda D_b$$\n",
    "\n",
    "Where, $\\lambda$ is the learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07c1362",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "869a7e5a",
   "metadata": {},
   "source": [
    "### Q1.c Generation of artificial data and estimation of lin reg parameters while using gradient descent method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be3066f",
   "metadata": {},
   "source": [
    "In this problem we are working with artificial data. And the model parameters are known to us. **Thus we have not splited the data into traing set and testing set**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6b8b4",
   "metadata": {},
   "source": [
    "In this problem we have to update parameter vector. So let us derive the update rule continue from Q1.b derivation.\n",
    "\n",
    "In Q1.b we have got update rule:\n",
    "$$a:=a- \\lambda D_a$$\n",
    "$$b:=b -\\lambda D_b$$\n",
    "\n",
    "Where,\n",
    "\n",
    "$\\begin{align*}\n",
    "    D_a & =\n",
    "     - \\frac{2}{n} \\sum_{i=1}^n x_i(y_i - y_{pred})\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "    D_b & =\n",
    "    - \\frac{2}{n} \\sum_{i=1}^n (y_i - y_{pred})\n",
    "\\end{align*}$\n",
    "\n",
    "Now can write,\n",
    "\n",
    "$\\begin{align*}\n",
    "     \\begin{pmatrix}b\\\\a\\end{pmatrix} & =\n",
    "     \\begin{pmatrix}b\\\\a\\end{pmatrix} - \\lambda \\begin{pmatrix} D_b\\\\D_a\\end{pmatrix} \\\\\n",
    "     & = \\begin{pmatrix}b\\\\a\\end{pmatrix} - \\lambda \\begin{pmatrix} - \\frac{2}{n} \\sum_{i=1}^n (y_i - y_{pred}) \\\\ - \\frac{2}{n} \\sum_{i=1}^n x_i(y_i - y_{pred}) \\end{pmatrix} \\\\\n",
    "     & = \\begin{pmatrix}b\\\\a\\end{pmatrix} - \\lambda^\\prime \\begin{pmatrix} \\sum_{i=1}^n (y_i - y_{pred}) \\\\  \\sum_{i=1}^n x_i(y_i - y_{pred}) \\end{pmatrix}   \\: Where, \\: \\lambda^\\prime = -\\frac{2\\lambda}{n} \\\\\n",
    "     & = \\begin{pmatrix}b\\\\a\\end{pmatrix} - \\lambda^\\prime   \\begin{pmatrix}1 & 1 & \\dots & 1 \\\\x_1 & x_2 & \\dots & x_n \\end{pmatrix}    \\begin{pmatrix} y_1 - y_{pred} \\\\ y_2 - y_{pred} \\\\ \\dots \\\\ y_n - y_{pred}\\end{pmatrix} \\\\\n",
    "     & = \\begin{pmatrix}b\\\\a\\end{pmatrix} - \\lambda^\\prime   X^t \\mathbf{y}\n",
    "\\end{align*}$\n",
    "\n",
    "Where, \n",
    "$\n",
    "X^t = \\begin{pmatrix}1 & 1 & \\dots & 1 \\\\x_1 & x_2 & \\dots & x_n \\end{pmatrix}\n",
    "$\n",
    "\n",
    "$\n",
    "\\mathbf{y} = \\begin{pmatrix} y_1 - y_{pred} \\\\ y_2 - y_{pred} \\\\ \\dots \\\\ y_n - y_{pred}\\end{pmatrix}\n",
    "$\n",
    "\n",
    "We will use the following relation to update parameter vector. For a given dataset $\\frac{-2}{n}$ is constant, so it is okay if we take it within $\\lambda$.\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7ff906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.1933505249999996 seconds to complete the batch gradient descent \n",
      "(intercept, gradient) = (b,a) = ([1.9926712], [[0.29946283]])\n"
     ]
    }
   ],
   "source": [
    "# generate artificial data\n",
    "# We are taking random seed as 17, this will help to regerate this random values\n",
    "np.random.seed(17)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # Arr of 10000 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # gen 10000 residual terms\n",
    "y = 2 + 0.3 * X + res\n",
    "\n",
    "\n",
    "# Start of batch gradient descent\n",
    "# Objective of this func is to calculate y label based on current parameter value\n",
    "def predict(X, weight):\n",
    "    return np.dot(X, weight) \n",
    "  \n",
    "# function to compute gradient of error function w.r.t. theta\n",
    "def gradient(X, y, weight):\n",
    "    y_pred = predict(X, weight)\n",
    "    grad = np.dot(X.transpose(), (y_pred - y)) \n",
    "    return grad\n",
    "\n",
    "\n",
    "# function to perform mini-batch gradient descent\n",
    "def gradientDescent_1c(X, y, learning_rate, tol=1e-06, n_iter = 1000):\n",
    "    # print(X.shape[1])\n",
    "    weight = np.zeros((X.shape[1], 1)) # Initialization of parameters with zeros\n",
    "    for _ in range(n_iter):\n",
    "        delta = - learning_rate * gradient(X, y, weight)\n",
    "        weight = weight + delta \n",
    "        if np.all(np.abs(delta) <= tol):\n",
    "            break\n",
    "    return weight\n",
    "\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "# print(X.shape)\n",
    "# print(X.ndim)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = y.reshape(-1,1)\n",
    "# print(y.shape)\n",
    "# print(y.ndim)\n",
    "\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1c(X, y, 0.00001)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the batch gradient descent \")\n",
    "\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4f93bc",
   "metadata": {},
   "source": [
    "### Q1.d Implementation of minibatch stochastic gradient descent "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bef9590",
   "metadata": {},
   "source": [
    "In mini-batch gradient descent we take a sub-set of sample from the whole set. Here we have been asked to implement minibatch stochastic gradient descent. We know that for batch size 1 mini-batch gradient descent is same as minibatch stochastic gradient descent.\n",
    "\n",
    "Here we have implemented mini-batch gradient descent in general. Our fuction takes batch-size as an argument. so we have passed batch_size = 1, to make it minibatch stochastic gradient descent.\n",
    "\n",
    "**Some specications of the entire code**\n",
    "\n",
    "- It has 4 functions\n",
    "- predict function calculate value of label, based on current parameter values\n",
    "- gradient function calculates the gradient (Theory is given in the context of Q1.c)\n",
    "- We have witten a function for generating a list of mini batches from the whole data set\n",
    "- Finally we have modified our previous function for performing mini-batch gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfd7028",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4bb706b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 1)\n",
      "It took 0.18038469700000004 seconds to complete the mini-batch gradient descent \n",
      "(intercept, gradient) = (b,a) = ([1.98845244], [[0.29530502]])\n"
     ]
    }
   ],
   "source": [
    "# generate artificial data\n",
    "np.random.seed(0)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # Arr of 10000 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # gen 10000 residual terms\n",
    "y = 2 + 0.3 * X + res\n",
    "\n",
    "# Start of batch gradient descent\n",
    "def predict(X, weight):\n",
    "    return np.dot(X, weight) # Returns dot product\n",
    "  \n",
    "# function to compute gradient of error function w.r.t. theta\n",
    "def gradient(X, y, weight):\n",
    "    y_pred = predict(X, weight)\n",
    "    grad = np.dot(X.transpose(), (y_pred - y)) \n",
    "    return grad\n",
    "\n",
    "# createing a list of mini-batches\n",
    "def create_mini_batches(X, y, batch_size):\n",
    "    mini_batches = []\n",
    "    data = np.hstack((X, y)) # grouping all of the data\n",
    "    np.random.shuffle(data) # make random shuffle of data\n",
    "    n_minibatches = data.shape[0] // batch_size # calculation of how many batch are there\n",
    "    i = 0\n",
    "  \n",
    "    for i in range(n_minibatches + 1):\n",
    "        mini_batch = data[i * batch_size:(i + 1)*batch_size, :]\n",
    "        X_mini = mini_batch[:, :-1] # dropping of last column\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1)) # take only the last col, and add an extra dim\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "        # If batch size is not multiple of data set size\n",
    "    if data.shape[0] % batch_size != 0:\n",
    "        mini_batch = data[i * batch_size:data.shape[0]]\n",
    "        X_mini = mini_batch[:, :-1]\n",
    "        Y_mini = mini_batch[:, -1].reshape((-1, 1))\n",
    "        mini_batches.append((X_mini, Y_mini))\n",
    "        \n",
    "    return mini_batches\n",
    "\n",
    "\n",
    "\n",
    "# function to perform mini-batch gradient descent\n",
    "def gradientDescent_1d(X, y, batch_size, learning_rate, tol=1e-06, epoch = 3):\n",
    "    # print(X.shape[1])\n",
    "    # Initialization of parameters with zeros\n",
    "    weight = np.zeros((X.shape[1], 1))\n",
    "    for _ in range(epoch):\n",
    "        mini_batches = create_mini_batches(X, y, batch_size)\n",
    "        for mini_batch in mini_batches:\n",
    "            X_mini, y_mini = mini_batch\n",
    "            delta = - learning_rate * gradient(X_mini, y_mini, weight)\n",
    "            weight = weight + delta\n",
    "            if np.all(np.abs(delta) <= tol):\n",
    "                break\n",
    "    return weight\n",
    "\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "print(X.shape)\n",
    "# print(X.ndim)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = y.reshape(-1,1)\n",
    "# print(y.shape)\n",
    "# print(y.ndim)\n",
    "\n",
    "# calculation of time\n",
    "tic = t.process_time()\n",
    "# The 4 parametrs are X the design matrix, labels, batch_size, learning_rate\n",
    "weight = gradientDescent_1d(X, y, 1, 0.0025) \n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch gradient descent \")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6024d2c9",
   "metadata": {},
   "source": [
    "### Q1.e Comparison of SGD (stochastic gradient descent) and mini-batch gradient descent with different batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2427832a",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "- **If we take _mini-batch size =1_ then it will be a stochastic gradient descent**\n",
    "- **If we taje _mini-batch size =(data-set size)_ it will be batch gradient descent**\n",
    "- For this problem we have taken batch size which is \n",
    "\n",
    "**Also Note:**\n",
    "Since for different computer processing time are different, so I have taken screenshots what result I have obtained and I am adding the screenshots in the directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bf94dcf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 0.3378917210000001 seconds to complete the mini-batch grad-des with batch_size = 1\n",
      "(intercept, gradient) = (b,a) = ([1.99866063], [[0.29739628]])\n",
      "\n",
      "\n",
      "It took 0.0673356260000002 seconds to complete the mini-batch grad-des with batch_size = 32\n",
      "(intercept, gradient) = (b,a) = ([1.90714364], [[0.28260718]])\n",
      "\n",
      "\n",
      "It took 0.044652854999999825 seconds to complete the mini-batch grad-des with batch_size = 64\n",
      "(intercept, gradient) = (b,a) = ([1.99066927], [[0.36515699]])\n",
      "\n",
      "\n",
      "It took 0.029542235000000083 seconds to complete the mini-batch grad-des with batch_size = 128\n",
      "(intercept, gradient) = (b,a) = ([2.01924215], [[0.33107668]])\n",
      "\n",
      "\n",
      "It took 0.025668881000000088 seconds to complete the mini-batch grad-des with batch_size = 256\n",
      "(intercept, gradient) = (b,a) = ([1.99220564], [[0.30210517]])\n",
      "\n",
      "\n",
      "It took 0.025707354999999765 seconds to complete the mini-batch grad-des with batch_size = 512\n",
      "(intercept, gradient) = (b,a) = ([1.98268469], [[0.26579349]])\n",
      "\n",
      "\n",
      "It took 0.026217388000000064 seconds to complete the mini-batch with batch_size = 1024\n",
      "(intercept, gradient) = (b,a) = ([1.9710702], [[0.28798408]])\n",
      "\n",
      "\n",
      "It took 0.029933994999999936 seconds to complete the mini-batch with batch_size = 2048\n",
      "(intercept, gradient) = (b,a) = ([1.88580559], [[0.31563096]])\n"
     ]
    }
   ],
   "source": [
    "# Here we will run gradientDescent_1d for different batch size\n",
    "np.random.seed(17)\n",
    "X = 2.5 * np.random.randn(10000) + 1.5 # Arr of 10000 values with mean = 1.5, stddev = 2.5\n",
    "res = 1.5 * np.random.randn(10000) # gen 10000 residual terms\n",
    "y = 2 + 0.3 * X + res\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# Here we have used any loop because for each batch size different learning rate was required \n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 1, 0.0025)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch grad-des with batch_size = 1\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 32, 0.0017)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch grad-des with batch_size = 32\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 64, 0.00161)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch grad-des with batch_size = 64\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 128, 0.0005)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch grad-des with batch_size = 128\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 256, 0.00035)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch grad-des with batch_size = 256\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 512, 0.00035)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch grad-des with batch_size = 512\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 1024, 0.000205)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch with batch_size = 1024\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")\n",
    "print(\"\\n\")\n",
    "\n",
    "tic = t.process_time()\n",
    "weight = gradientDescent_1d(X, y, 2048, 0.000105)\n",
    "toc = t.process_time()\n",
    "print( \"It took \" + str(toc - tic) + \" seconds to complete the mini-batch with batch_size = 2048\")\n",
    "print(\"(intercept, gradient) = (b,a) = \" + \"(\" + str(weight[0]) + \", \" + str(weight[1:]) + \")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bbca74",
   "metadata": {},
   "source": [
    "## Comment using the observed result (Q1e)\n",
    "\n",
    "- We make made the following table based on our observation.\n",
    "- For batch gradient descent we have used the result obtained in 1.c\n",
    "- Based on our observation **stochastic gradient descent** is the slowest.\n",
    "- Also batch gradient descent performed slower.\n",
    "- Mini-batch gradient descent with batch size 512, 1024, 2048 were the fastest three. \\[Not in order\\]\n",
    "- Batch size 256 also done quite well inters of both speed and parameter estimation accuracy\n",
    "- So we will recommend batch size 256 \\[Considering both speed and accuracy\\]\n",
    "\n",
    "| Batch Size | leaning_rate | Time Taken (in sec) | Cal. Val of b | Cal. Val of a | Comment         |\n",
    "|------------|--------------|---------------------|---------------|---------------|-----------------|\n",
    "| 1          | 0.0025       | 0.261499            | 1.998660      | 0.297396      |                 |\n",
    "| 32         | 0.0017       | 0.036875            | 1.907143      | 0.282607      |                 |\n",
    "| 64         | 0.00161      | 0.030282            | 1.990669      | 0.365156      |                 |\n",
    "| 128        | 0.0005       | 0.027439            | 2.019242      | 0.331076      |                 |\n",
    "| 256        | 0.00035      | 0.025568            | 1.992205      | 0.302105      |                 |\n",
    "| 512        | 0.00035      | 0.024917            | 1.982684      | 0.265793      |                 |\n",
    "| 1024       | 0.000205     | 0.024603            | 1.971070      | 0.287984      |                 |\n",
    "| 2048       | 0.000105     | 0.024488            | 1.885805      | 0.315630      |                 |\n",
    "| 10000      | 0.00001      | 0.097512            | 1.99267       | 0.299462      | Obtained in 1.c |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3b088a",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bea3b5",
   "metadata": {},
   "source": [
    "## Q2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46593157",
   "metadata": {},
   "source": [
    "### Q2(i) Probability that someone has both cold and a fever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15492d2d",
   "metadata": {},
   "source": [
    "Given,\n",
    "\n",
    "$Pr[Fever = T|Cold =T] = 0.307$\n",
    "\n",
    "and $Pr[Cold = T] = 0.02$\n",
    "\n",
    "$$\\therefore Pr[Fever = T,Cold =T] = Pr[Fever = T|Cold =T]Pr[Cold = T] = 0.307*0.02 = 0.00614$$ **(Ans)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5e996a",
   "metadata": {},
   "source": [
    "### Q2(ii) Probability that someone who has a cough has a cold \\[calculation in python also given\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01b9ae8",
   "metadata": {},
   "source": [
    "We will use the following notaion in this particular solution.\n",
    "\n",
    "- **Cou** to denote Cough.\n",
    "- **LD** to denote Lung Disease.\n",
    "- **Co** to denote cold. \n",
    "- **S** to denote Smoke.\n",
    "- **T** to denote True.\n",
    "- **F** to denote False.\n",
    "\n",
    "Here we have to find the probability, $$Pr[Co=T|Cou=T]$$\n",
    "\n",
    "Now,\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[Co=T|Cou=T] & =\n",
    "     \\frac{Pr[Co=T, Cou=T]}{Pr[Cou=T]} \\\\\n",
    "     & = \\frac{\\sum_{LD \\in \\{T,F\\}} Pr[Co=T, Cou=T, LD] }  {\\sum_{LD, \\; Co \\in \\{T,F\\}} Pr[Cou=T, Co, LD]}\\\\\n",
    "     & = \\frac{Pr[Cou=T, LD=T, Co=T] + Pr[Cou=T, LD=F, Co=T]}  {Pr[Cou=T, LD=T, Co=T] + Pr[Cou=T, LD=T, Co=F] + Pr[Cou=T, LD=F, Co=T] + Pr[Cou=T, LD=F, Co=F]}\n",
    "\\end{align*}$\n",
    "\n",
    "Before going into calculation of these probabilities we will calculate, $Pr[LD=T]$ and $Pr[LD=F]$ which will prove be necessary later.\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[LD=T] & =\n",
    "     Pr[LD=T, S=T] + Pr[LD=T, S=F] \\\\\n",
    "     & =  Pr[LD=T| S=T] Pr[S=T] + Pr[LD=T| S=F]Pr[S=F]\\\\\n",
    "     & =  0.1009 \\times 0.2 + 0.001 \\times 0.8 \\\\\n",
    "     & = 0.02098\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[LD=F] & =\n",
    "     1 - Pr[LD=T] = 0.97902\n",
    "\\end{align*}$\n",
    "\n",
    "Now calculate the following probabilities,\n",
    "\n",
    "**Note:** LD and Co are independent.\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[Cou=T, LD=T, Co=T] & =\n",
    "     Pr[Cou=T| LD=T, Co=T]Pr[LD=T, Co=T] \\\\\n",
    "     & = Pr[Cou=T| LD=T, Co=T]Pr[LD=T] Pr[Co=T] \\\\\n",
    "     & = 0.7525 \\times 0.02098 \\times 0.02 \\\\\n",
    "     & = 0.000315749\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[Cou=T, LD=T, Co=F] & =\n",
    "     Pr[Cou=T| LD=T, Co=F] Pr[LD=T, Co=F]\\\\\n",
    "     & =  Pr[Cou=T| LD=T, Co=F] Pr[LD=T] Pr[Co=F]\\\\\n",
    "     & = 0.505 \\times 0.02098 \\times 0.98 \\\\\n",
    "     & = 0.010383002\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[Cou=T, LD=F, Co=T] & =\n",
    "     Pr[Cou=T| LD=F, Co=T] Pr[LD=F, Co=T]\\\\\n",
    "     & =  Pr[Cou=T| LD=F, Co=T] Pr[LD=F] Pr[Co=T]\\\\\n",
    "     & = 0.505 \\times 0.97902 \\times 0.02\\\\\n",
    "     & = 0.009888102\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "     Pr[Cou=T, LD=F, Co=F] & =\n",
    "     Pr[Cou=T| LD=F, Co=F] Pr[LD=F, Co=F]\\\\\n",
    "     & =  Pr[Cou=T| LD=F, Co=F] Pr[LD=F] Pr[Co=F]\\\\\n",
    "     & = 0.01 \\times 0.97902 \\times 0.98\\\\\n",
    "     & = 0.009594396\n",
    "\\end{align*}$\n",
    "\n",
    "Putting the values in the above formuala we get, $$Pr[Co=T|Cou=T] = 0.338085776$$ **(Ans)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5ae78bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.020980000000000002\n",
      "0.000315749\n",
      "0.010383001999999999\n",
      "0.009888102\n",
      "0.009594396\n",
      "0.33808577637062004\n"
     ]
    }
   ],
   "source": [
    "# Rough calculation for Q2(ii)\n",
    "# Pr(LD=T) = p1\n",
    "p1 = (0.1009*0.2) + (0.001*0.8)\n",
    "# Pr(LD=F) = p2\n",
    "p2 = 1 - p1\n",
    "print(p1)\n",
    "\n",
    "# Pr[Cou=T, LD=T, Co=T]\n",
    "p3 = 0.7525 * 0.02098 * 0.02\n",
    "print(p3)\n",
    "\n",
    "# Pr[Cou=T, LD=T, Co=F]\n",
    "p4 = 0.505 * 0.02098 * 0.98\n",
    "print(p4)\n",
    "\n",
    "#Pr[Cou=T, LD=F, Co=T]\n",
    "p5 = 0.505 * 0.97902 * 0.02\n",
    "print(p5)\n",
    "\n",
    "#Pr[Cou=T, LD=F, Co=F]\n",
    "p6 = 0.01 * 0.97902 * 0.98\n",
    "print(p6)\n",
    "\n",
    "print((p3 + p5)/(p3 + p4 + p5 + p6))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867b5a3",
   "metadata": {},
   "source": [
    "## Q3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416e014f",
   "metadata": {},
   "source": [
    "### Calculation of maximum likelyhood estimate (MLE) of parameters of multionomial distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9df955f",
   "metadata": {},
   "source": [
    "Here we have to find the MLE estimate of multinomial distribution.\n",
    "\n",
    "Let us consider we have $n$ random variables $X_1, X_2, \\dots, X_n$.\n",
    "\n",
    "This variables are jointly distributed as multinomial distribution with probability $p_1, p_2, \\dots, p_n$ respectively. \n",
    "\n",
    "Clearly, $$p_1 + p_2 + \\dots + p_n =1$$\n",
    "\n",
    "Hence, their jouni pmf is given by, $$f(x_1, x_2, \\dots, x_n; p_1, p_2, \\dots, p_n) = \\frac{(x_1+x_2+ \\dots + x_n)!}{x_1!x_2! \\dots x_n!} p_1^{x_1} p_2^{x_2} \\dots p_n^{x_n}$$\n",
    "\n",
    "**Now consider that we have the following data.**\n",
    "\n",
    "Also, consider a particular realization of $N$ trails, where these random variables has occurred $x_1, x_2, \\dots, x_n$ times.\n",
    "\n",
    "clearly, $$x_1 + x_2 + \\dots + x_n = N$$\n",
    "\n",
    "So, the likelihood function is given by, $$\\mathcal{L}(p_1, p_2, \\dots, p_n|x_1, x_2, \\dots, x_n) = \\frac{N!}{x_1!x_2! \\dots x_n!} p_1^{x_1} p_2^{x_2} \\dots p_n^{x_n} $$\n",
    "\n",
    "Or the log-likelihood is given, $$\\mathcal{l}(p_1, p_2, \\dots, p_n|x_1, x_2, \\dots, x_n) = \\log(N!) - \\sum_{i=1}^n \\log(x_i!) + \\sum_{i=1}^n x_i \\log(p_i)$$\n",
    "\n",
    "We would like to maximize the log-likelihood wrt to the parameters $p_1, p_2, \\dots, p_n$ and subject to the constraint $p_1 + p_2 + \\dots + p_n =1$\n",
    "\n",
    "Or we would like to maximize, $$F(p_1, p_2, \\dots, p_n, \\lambda)=\\log(N!) - \\sum_{i=1}^n \\log(x_i!) + \\sum_{i=1}^n x_i \\log(p_i) + \\lambda (1-\\sum_{i=1}^n p_i)$$\n",
    "Where $\\lambda$ is Lagrange multiplyer\n",
    "\n",
    "Now,\n",
    "\n",
    "$\\begin{align*}\n",
    "    \\frac{\\partial F}{\\partial p_i} & =\n",
    "     \\frac{x_i}{p_i} - \\lambda , \\forall i = 1(1)n\n",
    "\\end{align*}$\n",
    "\n",
    "$\\begin{align*}\n",
    "    \\frac{\\partial F}{\\partial \\lambda} & =\n",
    "     1-\\sum_{i=1}^n p_i\n",
    "\\end{align*}$\n",
    "\n",
    "Now making, $$\\frac{\\partial F}{\\partial p_1}=\\frac{\\partial F}{\\partial p_2}= \\dots = \\frac{\\partial F}{\\partial p_n} = \\frac{\\partial F}{\\partial \\lambda} = 0$$\n",
    "\n",
    "We get,\n",
    "\n",
    "$$\\begin{cases}\n",
    "                      \\frac{x_i}{p_i} - \\lambda=0, \\forall i = 1(1)n \\\\\n",
    "                      1-\\sum_{i=1}^n p_i = 0\n",
    " \\end{cases}$$\n",
    "\n",
    "Or,\n",
    "\n",
    "$$\\begin{cases}\n",
    "                      p_i = \\frac{x_i}{\\lambda}, \\forall i = 1(1)n \\\\\n",
    "                      1-\\sum_{i=1}^n p_i = 0\n",
    " \\end{cases}$$\n",
    "\n",
    "Or,\n",
    "\n",
    "$$\\begin{cases}\n",
    "                      p_i = \\frac{x_i}{\\lambda}, \\forall i = 1(1)n \\\\\n",
    "                      \\lambda = \\sum_{i=1}^n x_i  = N\n",
    " \\end{cases}$$\n",
    " \n",
    "Hence, the maximum likelihood estimate of parameters are given by, $$(\\hat{p_1}, \\hat{p_2}, \\dots, \\hat{p_n}) = (\\frac{x_1}{N}, \\frac{x_2}{N}, \\dots, \\frac{x_n}{N}) = (\\frac{x_1}{\\sum_{i=1}^n x_i}, \\frac{x_2}{\\sum_{i=1}^n x_i}, \\dots, \\frac{x_n}{\\sum_{i=1}^n x_i})$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07466be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22de1218",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c55e9ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
